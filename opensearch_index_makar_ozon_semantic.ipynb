{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSearch: семантическое чанкирование и индекс `makar_ozon_semantic`\n",
    "\n",
    "Этот ноутбук:\n",
    "- берёт markdown‑документы из папки `Документация 2`,\n",
    "- режет их на предложения и объединяет предложения в чанки по косинусному сходству эмбеддингов,\n",
    "- считает эмбеддинги через Yandex textEmbedding API,\n",
    "- создаёт индекс `makar_ozon_semantic` и индексирует в него все чанки.\n",
    "\n",
    "Индекс потом можно использовать в сервисе `/search`, `/rag/answer` и в ноутбуке `opensearch_eval_colbert.ipynb` (через `index_name=\"makar_ozon_semantic\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T12:42:09.912756Z",
     "iopub.status.busy": "2025-12-17T12:42:09.912366Z",
     "iopub.status.idle": "2025-12-17T12:42:10.069264Z",
     "shell.execute_reply": "2025-12-17T12:42:10.068599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenSearch: https://localhost:9200 index: makar_ozon_semantic\n",
      "MD_DIR: ./docs\n",
      "Yandex folder: b1g97ml4ut1e2ar8iuaj\n",
      "Yandex embed model: text-search-doc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "MD_DIR = \"./docs\"\n",
    "INDEX_NAME = \"makar_ozon_semantic\"\n",
    "\n",
    "OPENSEARCH_URL = os.getenv(\"OPENSEARCH_URL\")\n",
    "OPENSEARCH_USER = os.getenv(\"OPENSEARCH_USER\")\n",
    "OPENSEARCH_PASSWORD = os.getenv(\"OPENSEARCH_PASSWORD\")\n",
    "\n",
    "if not OPENSEARCH_URL:\n",
    "    raise ValueError(\"OPENSEARCH_URL must be set in environment variables\")\n",
    "if not OPENSEARCH_USER:\n",
    "    raise ValueError(\"OPENSEARCH_USER must be set in environment variables\")\n",
    "if not OPENSEARCH_PASSWORD:\n",
    "    raise ValueError(\"OPENSEARCH_PASSWORD must be set in environment variables\")\n",
    "\n",
    "YANDEX_API_KEY = os.getenv(\"YANDEX_API_KEY\")\n",
    "YANDEX_FOLDER_ID = os.getenv(\"YANDEX_FOLDER_ID\")\n",
    "\n",
    "if not YANDEX_API_KEY:\n",
    "    raise ValueError(\"YANDEX_API_KEY must be set in environment variables\")\n",
    "if not YANDEX_FOLDER_ID:\n",
    "    raise ValueError(\"YANDEX_FOLDER_ID must be set in environment variables\")\n",
    "YANDEX_EMBED_MODEL = os.getenv(\"YANDEX_EMBED_MODEL\", \"text-search-doc\")\n",
    "YANDEX_EMBEDDINGS_URL = os.getenv(\n",
    "    \"YANDEX_EMBEDDINGS_URL\",\n",
    "    \"https://llm.api.cloud.yandex.net/foundationModels/v1/textEmbedding\",\n",
    ")\n",
    "\n",
    "SIM_THRESHOLD = float(os.getenv(\"SEMANTIC_SIM_THRESHOLD\", \"0.8\"))\n",
    "MAX_SENT_PER_CHUNK = int(os.getenv(\"MAX_SENT_PER_CHUNK\", \"8\"))\n",
    "\n",
    "print(\"OpenSearch:\", OPENSEARCH_URL, \"index:\", INDEX_NAME)\n",
    "print(\"MD_DIR:\", MD_DIR)\n",
    "print(\"Yandex folder:\", YANDEX_FOLDER_ID)\n",
    "print(\"Yandex embed model:\", YANDEX_EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T12:42:10.098864Z",
     "iopub.status.busy": "2025-12-17T12:42:10.098646Z",
     "iopub.status.idle": "2025-12-17T12:42:10.139415Z",
     "shell.execute_reply": "2025-12-17T12:42:10.138696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenSearch client ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vklepov/Documents/opensearch-service/.venv/lib/python3.13/site-packages/opensearchpy/connection/http_requests.py:161: UserWarning: Connecting to https://localhost:9200 using SSL with verify_certs=False is insecure.\n",
      "  warnings.warn(\n",
      "/Users/vklepov/Documents/opensearch-service/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "\n",
    "def create_client(url: str, user: str, password: str, delay: int) -> OpenSearch:\n",
    "    auth = HTTPBasicAuth(user, password) if user and password else None\n",
    "    client = OpenSearch(\n",
    "        hosts=[url],\n",
    "        http_compress=True,\n",
    "        http_auth=auth,\n",
    "        use_ssl=url.startswith(\"https://\"),\n",
    "        verify_certs=False,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        timeout=60,\n",
    "        max_retries=3,\n",
    "        retry_on_timeout=True,\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            health_status = client.cluster.health(wait_for_status=\"yellow\", timeout=5)\n",
    "            if health_status[\"status\"] in [\"yellow\", \"green\"]:\n",
    "                print(\"OpenSearch client ready\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"OpenSearch not ready yet: {e}. Retrying in {delay} seconds...\")\n",
    "        sleep(delay)\n",
    "    return client\n",
    "\n",
    "\n",
    "client = create_client(OPENSEARCH_URL, OPENSEARCH_USER, OPENSEARCH_PASSWORD, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T12:42:10.141703Z",
     "iopub.status.busy": "2025-12-17T12:42:10.141546Z",
     "iopub.status.idle": "2025-12-17T12:42:10.254167Z",
     "shell.execute_reply": "2025-12-17T12:42:10.253304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yandex embedding dim: 256\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not (YANDEX_API_KEY and YANDEX_FOLDER_ID):\n",
    "    raise ValueError(\n",
    "        \"YANDEX_API_KEY and YANDEX_FOLDER_ID must be set in environment variables\"\n",
    "    )\n",
    "\n",
    "MODEL_URI = f\"emb://{YANDEX_FOLDER_ID}/{YANDEX_EMBED_MODEL}/latest\"\n",
    "\n",
    "\n",
    "def yandex_embed_one(text: str) -> List[float]:\n",
    "    body = {\"modelUri\": MODEL_URI, \"text\": text}\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Api-Key {YANDEX_API_KEY}\",\n",
    "        \"x-folder-id\": YANDEX_FOLDER_ID,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    resp = requests.post(YANDEX_EMBEDDINGS_URL, headers=headers, json=body, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    emb = data.get(\"embedding\") or (data.get(\"result\") or {}).get(\"embedding\")\n",
    "    if emb is None:\n",
    "        raise RuntimeError(f\"Bad embedding response: {data}\")\n",
    "    return emb\n",
    "\n",
    "\n",
    "_test_vec = yandex_embed_one(\"тестовая строка для определения размерности\")\n",
    "EMBED_DIM = len(_test_vec)\n",
    "print(\"Yandex embedding dim:\", EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T12:42:10.256364Z",
     "iopub.status.busy": "2025-12-17T12:42:10.256183Z",
     "iopub.status.idle": "2025-12-17T12:42:10.356432Z",
     "shell.execute_reply": "2025-12-17T12:42:10.355759Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vklepov/Documents/opensearch-service/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/Users/vklepov/Documents/opensearch-service/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/Users/vklepov/Documents/opensearch-service/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index makar_ozon_semantic exists. Deleting...\n",
      "Creating index makar_ozon_semantic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created\n"
     ]
    }
   ],
   "source": [
    "INDEX_BODY: Dict[str, Any] = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0,\n",
    "            \"knn\": True,\n",
    "            \"knn.algo_param.ef_search\": 100,\n",
    "            \"similarity\": {\n",
    "                \"custom_similarity\": {\n",
    "                    \"type\": \"BM25\",\n",
    "                    \"k1\": 1.2,\n",
    "                    \"b\": 0.75,\n",
    "                    \"discount_overlaps\": \"true\",\n",
    "                }\n",
    "            },\n",
    "            \"analysis\": {\n",
    "                \"filter\": {\n",
    "                    \"russian_stemmer\": {\"type\": \"stemmer\", \"language\": \"russian\"},\n",
    "                    \"unique_pos\": {\"type\": \"unique\", \"only_on_same_position\": False},\n",
    "                    \"my_multiplexer\": {\n",
    "                        \"type\": \"multiplexer\",\n",
    "                        \"filters\": [\n",
    "                            \"keyword_repeat\",\n",
    "                            \"russian_stemmer\",\n",
    "                            \"remove_duplicates\",\n",
    "                        ],\n",
    "                    },\n",
    "                },\n",
    "                \"analyzer\": {\n",
    "                    \"search_text_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"standard\",\n",
    "                        \"filter\": [\"lowercase\", \"my_multiplexer\", \"unique_pos\"],\n",
    "                        \"char_filter\": [\"e_mapping\"],\n",
    "                    },\n",
    "                    \"ru_international_translit_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"standard\",\n",
    "                        \"filter\": [\n",
    "                            \"lowercase\",\n",
    "                            \"russian_stemmer\",\n",
    "                        ],\n",
    "                        \"char_filter\": [\"transliteration_filter\", \"e_mapping\"],\n",
    "                    },\n",
    "                    \"text_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"standard\",\n",
    "                        \"filter\": [\n",
    "                            \"lowercase\",\n",
    "                            \"russian_stemmer\",\n",
    "                        ],\n",
    "                        \"char_filter\": [\"e_mapping\"],\n",
    "                    },\n",
    "                    \"exact_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"standard\",\n",
    "                        \"filter\": [\"lowercase\"],\n",
    "                        \"char_filter\": [\"e_mapping\"],\n",
    "                    },\n",
    "                    \"text_standard\": {\"type\": \"standard\"},\n",
    "                    \"text_whitespace\": {\"type\": \"whitespace\"},\n",
    "                    \"text_lowercase\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"standard\",\n",
    "                        \"filter\": [\"lowercase\"],\n",
    "                    },\n",
    "                },\n",
    "                \"char_filter\": {\n",
    "                    \"transliteration_filter\": {\n",
    "                        \"type\": \"mapping\",\n",
    "                        \"mappings\": [\n",
    "                            \"a => а\",\n",
    "                            \"b => б\",\n",
    "                            \"v => в\",\n",
    "                            \"g => г\",\n",
    "                            \"d => д\",\n",
    "                            \"e => е\",\n",
    "                            \"ye => ё\",\n",
    "                            \"zh => ж\",\n",
    "                            \"z => з\",\n",
    "                            \"i => и\",\n",
    "                            \"j => й\",\n",
    "                            \"k => к\",\n",
    "                            \"l => л\",\n",
    "                            \"m => м\",\n",
    "                            \"n => н\",\n",
    "                            \"o => о\",\n",
    "                            \"p => п\",\n",
    "                        ],\n",
    "                    },\n",
    "                    \"e_mapping\": {\"type\": \"mapping\", \"mappings\": [\"e => ё\"]},\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"text_analyzer\",\n",
    "                \"similarity\": \"BM25\",\n",
    "            },\n",
    "            \"source\": {\"type\": \"keyword\"},\n",
    "            \"chunk_id\": {\"type\": \"keyword\"},\n",
    "            \"text_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": EMBED_DIM,\n",
    "                \"space_type\": \"cosinesimil\",\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"parameters\": {\"ef_construction\": 512, \"m\": 64},\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "if client.indices.exists(index=INDEX_NAME):\n",
    "    print(f\"Index {INDEX_NAME} exists. Deleting...\")\n",
    "    client.indices.delete(index=INDEX_NAME)\n",
    "\n",
    "print(f\"Creating index {INDEX_NAME}...\")\n",
    "client.indices.create(index=INDEX_NAME, body=INDEX_BODY)\n",
    "print(\"Index created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T12:42:10.358289Z",
     "iopub.status.busy": "2025-12-17T12:42:10.358135Z",
     "iopub.status.idle": "2025-12-17T12:42:10.368578Z",
     "shell.execute_reply": "2025-12-17T12:42:10.368012Z"
    }
   },
   "outputs": [],
   "source": [
    "SENT_SPLIT_REGEX = re.compile(r\"([.!?]+)\\s+\")\n",
    "\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    parts = SENT_SPLIT_REGEX.split(text)\n",
    "    sentences: List[str] = []\n",
    "    buf = \"\"\n",
    "    for part in parts:\n",
    "        if not part:\n",
    "            continue\n",
    "        if SENT_SPLIT_REGEX.match(part):\n",
    "            buf += part + \" \"\n",
    "            sentences.append(buf.strip())\n",
    "            buf = \"\"\n",
    "        else:\n",
    "            buf += part + \" \"\n",
    "    if buf.strip():\n",
    "        sentences.append(buf.strip())\n",
    "    return [s for s in sentences if s.strip()]\n",
    "\n",
    "\n",
    "def build_semantic_chunks_for_file(path: str, fname: str) -> List[Dict[str, Any]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    paragraphs = [p.strip() for p in content.split(\"\\n\\n\") if p.strip()]\n",
    "    sentences: List[str] = []\n",
    "    for p in paragraphs:\n",
    "        sentences.extend(split_into_sentences(p))\n",
    "\n",
    "    if not sentences:\n",
    "        return []\n",
    "\n",
    "    sent_embs = []\n",
    "    for s in sentences:\n",
    "        sent_embs.append(np.array(yandex_embed_one(s), dtype=\"float32\"))\n",
    "    sent_embs = np.stack(sent_embs, axis=0)\n",
    "\n",
    "    chunk_spans: List[tuple] = []\n",
    "\n",
    "    cur_indices: List[int] = [0]\n",
    "    cur_vec = sent_embs[0].copy()\n",
    "\n",
    "    for i in range(1, len(sentences)):\n",
    "        vec = sent_embs[i]\n",
    "        num = float(np.dot(cur_vec, vec))\n",
    "        den = float(np.linalg.norm(cur_vec) * np.linalg.norm(vec))\n",
    "        sim = num / den if den != 0.0 else 0.0\n",
    "\n",
    "        if sim >= SIM_THRESHOLD and len(cur_indices) < MAX_SENT_PER_CHUNK:\n",
    "            cur_indices.append(i)\n",
    "            cur_vec = sent_embs[cur_indices].mean(axis=0)\n",
    "        else:\n",
    "            start, end = cur_indices[0], cur_indices[-1]\n",
    "            chunk_spans.append((start, end))\n",
    "            cur_indices = [i]\n",
    "            cur_vec = vec.copy()\n",
    "\n",
    "    if cur_indices:\n",
    "        start, end = cur_indices[0], cur_indices[-1]\n",
    "        chunk_spans.append((start, end))\n",
    "\n",
    "    merged_spans: List[tuple] = []\n",
    "    i = 0\n",
    "    while i < len(chunk_spans):\n",
    "        start, end = chunk_spans[i]\n",
    "        sent_count = end - start + 1\n",
    "\n",
    "        if sent_count < 3 and i + 1 < len(chunk_spans):\n",
    "            next_start, next_end = chunk_spans[i + 1]\n",
    "            merged_spans.append((start, next_end))\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_spans.append((start, end))\n",
    "            i += 1\n",
    "\n",
    "    if len(merged_spans) >= 2:\n",
    "        last_start, last_end = merged_spans[-1]\n",
    "        if (last_end - last_start + 1) < 3:\n",
    "            prev_start, prev_end = merged_spans[-2]\n",
    "            merged_spans[-2] = (prev_start, last_end)\n",
    "            merged_spans.pop()\n",
    "\n",
    "    chunks: List[Dict[str, Any]] = []\n",
    "    for start, end in merged_spans:\n",
    "        idxs = list(range(start, end + 1))\n",
    "        text = \" \".join(sentences[j] for j in idxs)\n",
    "        vec = sent_embs[idxs].mean(axis=0)\n",
    "        chunk_id = f\"{fname}::s{start}-{end}\"\n",
    "        chunks.append(\n",
    "            {\n",
    "                \"text\": text,\n",
    "                \"source\": fname,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text_vector\": vec.tolist(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T12:42:10.371019Z",
     "iopub.status.busy": "2025-12-17T12:42:10.370841Z",
     "iopub.status.idle": "2025-12-17T12:42:32.040079Z",
     "shell.execute_reply": "2025-12-17T12:42:32.038196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading zip from https://storage.yandexcloud.net/callcenter-rag/docs.zip...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting zip file...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted to: /Users/vklepov/Documents/opensearch-service/docs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atm.md: 1 semantic chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank-cards-and-operations.md: 4 semantic chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business-accuonts.md: 3 semantic chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complaints-and-incident-escalation.md: 4 semantic chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit-and-restructurization.md: 3 semantic chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currency-operations-and-accounts.md: 2 semantic chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deposits-and-savings.md: 3 semantic chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fradulent-and-suspicios-operations.md: 3 semantic chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identifications-and-antifraud.md: 2 semantic chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobile-banking.md: 2 semantic chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mortgages-and-real-estate.md: 1 semantic chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "online-banking.md: 3 semantic chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payments-and-transfers.md: 1 semantic chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salary-and-bulk-payments.md: 3 semantic chunks\n",
      "Total semantic chunks: 35\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import io\n",
    "\n",
    "\n",
    "def download_and_extract_zip(url, extract_to):\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    try:\n",
    "        print(f\"Downloading zip from {url}...\")\n",
    "\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        print(\"Extracting zip file...\")\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "            for item in zip_ref.infolist():\n",
    "                if item.filename.startswith(\"__MACOSX\") or \".DS_Store\" in item.filename:\n",
    "                    continue\n",
    "                zip_ref.extract(item, extract_to)\n",
    "\n",
    "        print(f\"Extracted to: {os.path.abspath(extract_to)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "ZIP_URL = os.getenv(\"DOCS_ZIP_URL\")\n",
    "download_and_extract_zip(ZIP_URL, MD_DIR)\n",
    "\n",
    "md_files = [fn for fn in os.listdir(MD_DIR) if fn.lower().endswith(\".md\")]\n",
    "md_files.sort()\n",
    "\n",
    "docs: List[Dict[str, Any]] = []\n",
    "for fname in md_files:\n",
    "    path = os.path.join(MD_DIR, fname)\n",
    "    file_chunks = build_semantic_chunks_for_file(path, fname)\n",
    "    print(f\"{fname}: {len(file_chunks)} semantic chunks\")\n",
    "    docs.extend(file_chunks)\n",
    "\n",
    "print(\"Total semantic chunks:\", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T12:42:32.116799Z",
     "iopub.status.busy": "2025-12-17T12:42:32.116631Z",
     "iopub.status.idle": "2025-12-17T12:42:32.210360Z",
     "shell.execute_reply": "2025-12-17T12:42:32.209635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulk indexed 35 semantic chunks into 'makar_ozon_semantic'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vklepov/Documents/opensearch-service/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "BULK_ENDPOINT = f\"/{INDEX_NAME}/_bulk\"\n",
    "lines: List[str] = []\n",
    "\n",
    "for i, d in enumerate(docs):\n",
    "    doc_id = d.get(\"chunk_id\") or f\"doc-{i}\"\n",
    "    meta = {\"index\": {\"_index\": INDEX_NAME, \"_id\": doc_id}}\n",
    "    src = {\n",
    "        \"text\": d[\"text\"],\n",
    "        \"source\": d[\"source\"],\n",
    "        \"chunk_id\": d[\"chunk_id\"],\n",
    "        \"text_vector\": d[\"text_vector\"],\n",
    "    }\n",
    "    lines.append(json.dumps(meta, ensure_ascii=False))\n",
    "    lines.append(json.dumps(src, ensure_ascii=False))\n",
    "\n",
    "print(\"Starting bulk index...\")\n",
    "payload = \"\\n\".join(lines) + \"\\n\"\n",
    "resp = client.transport.perform_request(\"POST\", BULK_ENDPOINT, body=payload)\n",
    "if isinstance(resp, dict) and resp.get(\"errors\"):\n",
    "    errs = sum(\n",
    "        1 for it in resp.get(\"items\", []) if (it.get(\"index\") or {}).get(\"error\")\n",
    "    )\n",
    "    print(\"Bulk completed with errors:\", errs)\n",
    "else:\n",
    "    print(f\"Bulk indexed {len(docs)} semantic chunks into '{INDEX_NAME}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T12:42:32.212755Z",
     "iopub.status.busy": "2025-12-17T12:42:32.212574Z",
     "iopub.status.idle": "2025-12-17T12:42:47.861399Z",
     "shell.execute_reply": "2025-12-17T12:42:47.859963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего чанков: 35. Будем генерировать вопросы для 35 случайных чанков.\n",
      "[22] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15] generating question…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 35 query–chunk pairs\n",
      "Saved to test_queries_semantic.json\n"
     ]
    }
   ],
   "source": [
    "# import random\n",
    "\n",
    "# OUT_QUERIES_PATH = \"test_queries_semantic.json\"\n",
    "# N_QUESTIONS = 100\n",
    "\n",
    "# YANDEX_LLM_MODEL = os.getenv(\"YANDEX_LLM_MODEL\", \"yandexgpt-lite\")\n",
    "# YANDEX_COMPLETION_URL = os.getenv(\n",
    "#     \"YANDEX_COMPLETION_URL\",\n",
    "#     \"https://llm.api.cloud.yandex.net/foundationModels/v1/completion\",\n",
    "# )\n",
    "\n",
    "# if not (YANDEX_API_KEY and YANDEX_FOLDER_ID):\n",
    "#     raise ValueError(\n",
    "#         \"YANDEX_API_KEY and YANDEX_FOLDER_ID must be set in environment variables\"\n",
    "#     )\n",
    "\n",
    "# llm_headers = {\n",
    "#     \"Authorization\": f\"Api-Key {YANDEX_API_KEY}\",\n",
    "#     \"x-folder-id\": YANDEX_FOLDER_ID,\n",
    "#     \"Content-Type\": \"application/json\",\n",
    "# }\n",
    "\n",
    "\n",
    "# def generate_question_for_chunk(text: str) -> str:\n",
    "#     \"\"\"Генерирует один естественный пользовательский вопрос по смыслу данного чанка.\"\"\"\n",
    "#     snippet = text.strip().replace(\"\\n\", \" \")\n",
    "#     if len(snippet) > 1200:\n",
    "#         snippet = snippet[:1200]\n",
    "\n",
    "#     prompt = (\n",
    "#         \"Ты помогаешь составлять тестовые пользовательские запросы по текстам.\\n\"\n",
    "#         \"Ниже дан фрагмент текста (чанк). Сформулируй ОДИН естественный, \"\n",
    "#         \"конкретный вопрос на русском языке, который пользователь мог бы задать, \"\n",
    "#         \"чтобы найти именно этот фрагмент. Не пиши ничего, кроме самого вопроса.\\n\\n\"\n",
    "#         f\"Текст чанка:\\n{snippet}\\n\\n\"\n",
    "#         \"Вопрос:\"\n",
    "#     )\n",
    "\n",
    "#     body = {\n",
    "#         \"modelUri\": f\"gpt://{YANDEX_FOLDER_ID}/{YANDEX_LLM_MODEL}/latest\",\n",
    "#         \"completionOptions\": {\n",
    "#             \"stream\": False,\n",
    "#             \"temperature\": 0.4,\n",
    "#             \"maxTokens\": 120,\n",
    "#         },\n",
    "#         \"messages\": [\n",
    "#             {\"role\": \"user\", \"text\": prompt},\n",
    "#         ],\n",
    "#     }\n",
    "\n",
    "#     resp = requests.post(\n",
    "#         YANDEX_COMPLETION_URL,\n",
    "#         headers=llm_headers,\n",
    "#         json=body,\n",
    "#         timeout=60,\n",
    "#     )\n",
    "#     resp.raise_for_status()\n",
    "#     data = resp.json()\n",
    "\n",
    "#     try:\n",
    "#         text_out = data[\"result\"][\"alternatives\"][0][\"message\"][\"text\"].strip()\n",
    "#     except Exception as e:\n",
    "#         raise RuntimeError(f\"Bad completion response: {data}\") from e\n",
    "\n",
    "#     return text_out\n",
    "\n",
    "\n",
    "# if not docs:\n",
    "#     raise RuntimeError(\n",
    "#         \"Список docs пуст — сначала собери семантические чанки (ячейки выше)\"\n",
    "#     )\n",
    "\n",
    "# num_available = len(docs)\n",
    "# num_to_sample = min(N_QUESTIONS, num_available)\n",
    "# all_indices = list(range(num_available))\n",
    "# random.shuffle(all_indices)\n",
    "# sample_indices = all_indices[:num_to_sample]\n",
    "\n",
    "# print(\n",
    "#     f\"Всего чанков: {num_available}. Будем генерировать вопросы для {num_to_sample} случайных чанков.\"\n",
    "# )\n",
    "\n",
    "# results: List[Dict[str, Any]] = []\n",
    "# for idx in sample_indices:\n",
    "#     d = docs[idx]\n",
    "#     text = (d.get(\"text\") or \"\").strip()\n",
    "#     if not text:\n",
    "#         continue\n",
    "\n",
    "#     print(f\"[{idx}] generating question…\")\n",
    "#     try:\n",
    "#         q = generate_question_for_chunk(text)\n",
    "#     except Exception as e:  # noqa: BLE001\n",
    "#         print(f\"  error on chunk {idx}: {e}\")\n",
    "#         continue\n",
    "\n",
    "#     results.append(\n",
    "#         {\n",
    "#             \"id\": idx,\n",
    "#             \"question\": q,\n",
    "#             \"chunk_text\": text,\n",
    "#             \"source\": d.get(\"source\"),\n",
    "#             \"chunk_id\": d.get(\"chunk_id\"),\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "# print(f\"Generated {len(results)} query–chunk pairs\")\n",
    "\n",
    "# with open(OUT_QUERIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(f\"Saved to {OUT_QUERIES_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
